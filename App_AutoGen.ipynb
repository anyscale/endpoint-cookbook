{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e83a98ba-8b18-42a7-80a2-aa22d4663529",
   "metadata": {},
   "source": [
    "# AutoGen\n",
    "### AutoGen is a framework that enables the development of LLM applications using multiple agents that can converse with each other to solve tasks. \n",
    "### See more details at https://github.com/microsoft/autogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae430ef-687a-4c1a-916c-50cbc9885136",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Installation\n",
    "!pip install pyautogen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef94b5cb-2544-45fd-bf95-293bc3620e25",
   "metadata": {
    "tags": []
   },
   "source": [
    "### AutoGen provides a drop-in replacement of openai.ChatCompletion as an enhanced inference API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ede3a9-27c9-4d37-9532-6693692a90f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from autogen import oai\n",
    "from openai.openai_object import OpenAIObject\n",
    "\n",
    "ANYSCALE_ENDPOINT_API_KEY = \"esecret_XXXXX\"\n",
    "response :OpenAIObject = oai.ChatCompletion.create(\n",
    "    config_list=[\n",
    "        {\n",
    "            \"model\": \"codellama/CodeLlama-34b-Instruct-hf\",\n",
    "            \"api_type\": \"open_ai\",\n",
    "            \"api_base\": \"https://console.endpoints.anyscale.com/m/v1\",\n",
    "            \"api_key\": ANYSCALE_ENDPOINT_API_KEY,\n",
    "        }\n",
    "    ],\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are an AI expert\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a joke\"}\n",
    "    ],\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "content = response.get(\"choices\")[0].get(\"message\").get(\"content\")\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0db0bfa-1f60-4a99-88d6-536a9cbc2c68",
   "metadata": {},
   "source": [
    "### Let create a multi-agent chat application with AutoGen\n",
    "### 1, create a config list with your Anyscale Endpoint API Keys\n",
    "#### Use **codellama/CodeLlama-34b-Instruct-hf** model for the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f1978b6-9d60-4c51-86db-55401620ad2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "config_list = [\n",
    "    {\n",
    "        \"model\": \"codellama/CodeLlama-34b-Instruct-hf\",\n",
    "        \"api_type\": \"open_ai\",\n",
    "        \"api_base\": \"https://api.endpoints.anyscale.com/v1\",\n",
    "        \"api_key\": ANYSCALE_ENDPOINT_API_KEY\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4263bf-c42e-4cc5-94a0-2cfecc4703d3",
   "metadata": {},
   "source": [
    "### 2, create an LLM config with config list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb65c883-37ff-4ac2-9f92-1075692efd16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_config = {\n",
    "    \"request_timeout\": 600,\n",
    "    \"seed\": 45,  # change the seed for different trials\n",
    "    \"config_list\": config_list,\n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 2000,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6627865-85fb-4152-a185-31be3447c481",
   "metadata": {},
   "source": [
    "### 3, Create an agent assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0995e2e3-0b35-4d43-a4c0-160bd9027303",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config=llm_config,\n",
    "    is_termination_msg=lambda x: True if \"TERMINATE\" in x.get(\"content\") else False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450d9c2a-4b13-45e1-ac63-656af6b62720",
   "metadata": {},
   "source": [
    "### 4, Create a user_proxy\n",
    "#### To execute the code locally, you can use following **code_execution_config**,\n",
    "#### create a dummy **.dockenv** file  by `touch /.dockerenv`,\n",
    "#### and make sure **docker** is uninstalled by `pip uninstall docker`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a97b490-9657-4342-b03c-fa5bb6231dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set use_docker to False\n",
    "code_execution_config = {\"work_dir\": \"./local_test\", \"use_docker\": None}\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"TERMINATE\",\n",
    "    max_consecutive_auto_reply=1,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config=code_execution_config,\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"Reply TERMINATE if the task has been solved at full satisfaction.\n",
    "Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8904fc-56b7-49ab-b472-66fe23f458b9",
   "metadata": {},
   "source": [
    "### 5, Start your chat with AutoGen\n",
    "#### You can have human input at the end of each round of agent conversation\n",
    "#### and enter **exit** to finish the chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c2aa542-0707-4457-b57b-87b3e3421ff9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "What time is it right now? and how many hours till midnight?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "  To get the current time and calculate the hours till midnight, you can use the following code:\n",
      "```python\n",
      "import datetime\n",
      "\n",
      "# Get the current time\n",
      "now = datetime.datetime.now()\n",
      "\n",
      "# Calculate the hours till midnight\n",
      "hours_till_midnight = (24 - now.hour) % 24\n",
      "\n",
      "# Print the result\n",
      "print(f\"It is currently {now.strftime('%I:%M %p')}.\")\n",
      "print(f\"There are {hours_till_midnight} hours till midnight.\")\n",
      "```\n",
      "This code uses the `datetime` module to get the current time and calculate the hours till midnight. The `strftime` method is used to format the time in a human-readable format.\n",
      "\n",
      "You can execute this code in a Python interpreter or save it to a file and run it with the Python interpreter.\n",
      "\n",
      "Note: The `datetime` module is a part of the Python standard library, so you don't need to install any additional packages to use it.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "It is currently 06:59 PM.\n",
      "There are 6 hours till midnight.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "  Thank you for letting me know. I apologize for the confusion earlier. It's great to hear that the code executed successfully and provided the correct output. If you have any more questions or need further assistance, feel free to ask!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please give feedback to assistant. Press enter to skip and use auto-reply, or type 'exit' to stop the conversation:  exit\n"
     ]
    }
   ],
   "source": [
    "task1 = \"What time is it right now? and how many hours till midnight?\"\n",
    "user_proxy.initiate_chat(assistant,message=task1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
