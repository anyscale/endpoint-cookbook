{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a749a843-c604-44c6-abf0-906739597578",
   "metadata": {
    "tags": []
   },
   "source": [
    "# FireAct\n",
    "## FireAct takes an initial step to show multiple advantages of fine-tuning LMs for agentic uses. In this demo, we will use its method to finetune Llama 13B to generate multi queires which is used in [RAG-Fusion](https://github.com/anyscale/endpoint-cookbook/blob/main/App_RAG_Fusion.ipynb) demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e10556-60b6-49cd-ab45-223d28cbbea4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Even with prompt engineering, the Llama2 70B model can NOT generate the multi-queries we needed for RAG Fusion. The example below shows verbose reponse from Llama2 70B model.\n",
    "\n",
    "### Later, we will show a finetuned Llama2 7B model can achieve the purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26a87e5e-51e6-413f-aec7-36b831bdadb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sure, here are four search queries based on the original query \"What name is given to the explosive death of a star?\":',\n",
       " '',\n",
       " '* \"Types of star explosions\"',\n",
       " '* \"Stellar explosions and their names\"',\n",
       " '* \"Supernova vs. hypernova: what\\'s the difference?\"',\n",
       " '* \"The science behind a star\\'s explosive death\"',\n",
       " '',\n",
       " 'I hope these queries are helpful! Let me know if you need any further assistance.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import random\n",
    "\n",
    "msg=f\"Generates up to 5 search queries based on a single input query. You should generate queries relevent to the \\\n",
    "original one, output them in bullet points, be precise, no other verbose output needed. Here is the orignal \\\n",
    "query {original_query} and output (4 queries):\"\n",
    "\n",
    "def generate_queries_llama(original_query):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        api_base = \"https://console.endpoints.anyscale.com/m/v1\",\n",
    "        api_key = \"ANYSCALE_API_KEY\",\n",
    "        model=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "        messages=[{\"role\": \"user\", \"content\": msg}]\n",
    "    )\n",
    "\n",
    "    generated_queries = response.choices[0][\"message\"][\"content\"].strip().split(\"\\n\")\n",
    "    return generated_queries\n",
    "\n",
    "original_query = \"What name is given to the explosive death of a star?\"\n",
    "generated_queries = generate_queries_llama(original_query)\n",
    "generated_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e86043c-f0b8-48d3-81ff-1011c69f597e",
   "metadata": {},
   "source": [
    "### Now let's use the code provided in FireAct to generate training datasets and finetune Anyscale Endpoint. Here are the step:\n",
    "1. Clone the [repo](https://github.com/anchen1011/FireAct)  and add following contents  \n",
    "- `data/triviaqa/dev.json` (Download [TriviaQA](https://nlp.cs.washington.edu/triviaqa/) dataset)\n",
    "- `tasks/__init__.py` and `tasks/triviaqa.py` (work with TriviaQA datasets)\n",
    "- `prompts/triviaqa_multiqueries.txt` (few-shot examples for multi-query generation)\n",
    "2. Run `python generation-triviaqa.py` to generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7626aa9a-498e-4aec-b57e-9926dbaaafc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generation-triviaqa.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afeb616-9763-4b02-b47e-cb66900b4638",
   "metadata": {},
   "source": [
    "3. Training dataset will be generated under `trajs` folder. Convert from JSON to GPT-format JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137af90f-02c1-4d65-b6a6-270c09366ddd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "JSON_PATH = \"trajs/triviaqa_dev_0_300_gpt-4_0.0_2023-10-26-19-34-02.json\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "    items = json.load(f)\n",
    "\n",
    "sys_msg=\"Generates multiple search queries based on a single input query.\"\n",
    "outfile =  open('trivia.jsonl', 'w')\n",
    "entry={}\n",
    "for i, idx in enumerate(items):\n",
    "    item = items[idx] \n",
    "    entry['messages']=[]\n",
    "    entry['messages'].append({'role':'system','content':sys_msg})\n",
    "    entry['messages'].append({'role':'user','content':item[\"Query\"]})\n",
    "    entry['messages'].append({'role':'assistant','content':\"\\\\n\".join(item[\"Queries\"])})\n",
    "    \n",
    "    json.dump(entry, outfile)\n",
    "    outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aee614-0488-48ad-a230-8d7e4423a141",
   "metadata": {
    "tags": []
   },
   "source": [
    "4. Create training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da63aefb-622e-45e2-a304-9fb713e74c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"trivia.jsonl\"\n",
    "# Load the dataset\n",
    "with open(DATA_PATH, 'r') as f:\n",
    "    items = [json.loads(line) for line in f]\n",
    "threshold = int(len(items) * .85)\n",
    "with open('trivia_train.jsonl', 'w') as outfile:\n",
    "    for i, entry in enumerate(items):\n",
    "        if i<threshold:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "            \n",
    "with open('trivia_val.jsonl', 'w') as outfile:\n",
    "    for i, entry in enumerate(items):\n",
    "        if i>=threshold:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f75d2b4-5541-46ba-b0d2-94bf31eceddf",
   "metadata": {},
   "source": [
    "5. Finetune with Anyscale Endpoint Llama2 7B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340038de-e9d4-4a7f-8135-83985b80f677",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from finetune_utils import finetune_check\n",
    "finetune_check('./trivia_train.jsonl')\n",
    "finetune_check('./trivia_val.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60c739ac-a7d5-4b81-b4ac-d8bb178d1907",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from finetune_utils import finetune_run\n",
    "finetune_run('./trivia_train.jsonl','./trivia_val.jsonl',\n",
    "             token='ANYSCALE_API_TOKEN', \n",
    "             model='meta-llama/Llama-2-7b-chat-hf', \n",
    "             suffix='multiqueries')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95bd769-5787-4626-bd91-cc29704a2688",
   "metadata": {},
   "source": [
    "## Let's try to generate multi-queries again with the FT model and the results is exactly in the format of what we need and can be easily converted into Python List for further processings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beb22424-4225-490c-bdf7-c9077f81fe80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Explosive death of a star name\n",
      "- What is the technical term for exploding star?\n",
      "- What do we call the explosion of a star?\n",
      "- Name for the fatal star explosion\n",
      "- What is the scientific term for a star explosion?\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "def generate_queries_llama(original_query):\n",
    "    msg=f\"Generates up to 5 search queries based on a single input query. \\\n",
    "Here is the orignal query {original_query} and output (4 queries):\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        api_base = \"https://console.endpoints.anyscale.com/m/v1\",\n",
    "        api_key = \"ANYSCALE_API_KEY\",\n",
    "        model=\"meta-llama/Llama-2-7b-chat-hf:SUFFIX:ID\",\n",
    "        messages=[{\"role\": \"user\", \"content\": msg}]\n",
    "    )\n",
    "\n",
    "    generated_queries = response.choices[0][\"message\"][\"content\"].strip().split(\"\\\\n\")\n",
    "    return generated_queries\n",
    "\n",
    "original_query = \"What name is given to the explosive death of a star?\"\n",
    "generated_queries = generate_queries_llama(original_query)\n",
    "for generated_query in generated_queries:\n",
    "    print(generated_query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
