{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79baebb1",
   "metadata": {},
   "source": [
    "# Streaming Chatbot with Langchain\n",
    "##### Langchain is a model integration framework that allows to create applications using LLMs. This example creates a streaming chatbot with Langchain and requires the following packages: openai, langchain>=0.1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c4df44-8e29-4326-a0f2-61232347de85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# update to latest versions of openai and langchain\n",
    "!pip3 install -qU openai==1.3.6 langchain\n",
    "# print versions you got!\n",
    "!pip show openai\n",
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2debf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai  # works only for openai python package > 1.0.0 \n",
    "import time \n",
    "\n",
    "from dotenv import load_dotenv; load_dotenv()\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0b24bd-665a-4b0f-ac46-7fe03528a4ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Define your Chat Agent\n",
    "#### Handling streaming in LangChain requires responding to callbacks, which makes the program flow complicated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4fb005-eb38-4b9c-926e-075a90ff58ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatAnyscale\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "class LangchainChatAgent():\n",
    "\n",
    "    class StreamingCBH(BaseCallbackHandler):\n",
    "        def __init__(self, q):\n",
    "            self.q = q\n",
    "                \n",
    "        def on_llm_new_token(\n",
    "            self,\n",
    "            token,\n",
    "            *,\n",
    "            run_id,\n",
    "            parent_run_id = None,\n",
    "            **kwargs,\n",
    "        ) -> None:\n",
    "            self.q.put(token)\n",
    "        \n",
    "        def on_llm_end(self, response, *, run_id, parent_run_id, **kwargs):\n",
    "            self.q.put(INPUTMARKER_END)\n",
    "\n",
    "\n",
    "    def __init__(self, model: str = None):\n",
    "        #In this simple example, we do not modify the past conversation. \n",
    "        #Eventually you will run out of context window, but this should be enough for a 30-step conversation\n",
    "        #You need to either trim the message history or summarize it for longer conversations\n",
    "        self.message_history = ChatMessageHistory()\n",
    "        self.model = model\n",
    "        self.llm = ChatAnyscale(anyscale_api_key=ANYSCALE_ENDPOINT_TOKEN,\n",
    "                  temperature=0, model_name=self.model, streaming=True)\n",
    "        \n",
    "    def process_input(self, user_message: str):\n",
    "        self.message_history.add_user_message(user_message)\n",
    "        myq = Queue()\n",
    "\n",
    "        # Handling Streaming for LangChain with callbacks\n",
    "        # Kick off a thread that calls the predict_messages method, put tokens into the queue.\n",
    "        # Then in the main control program thread, we wait for elements to be pushed into the queue in the loop below.\n",
    "        thread =  Thread(target = self.llm.invoke, kwargs = \n",
    "                        {'input': self.message_history.messages,\n",
    "                         'config': {'callbacks':[self.StreamingCBH(myq)]}}\n",
    "                   )\n",
    "        thread.start() \n",
    "        ai_message = ''\n",
    "        while True:\n",
    "            token = myq.get()\n",
    "            if token == INPUTMARKER_END:\n",
    "                break\n",
    "            ai_message += token \n",
    "            yield token\n",
    "\n",
    "        self.message_history.add_ai_message(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec66d8ad-fa96-49c3-a1f5-05bf14903f01",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Instantiate a chat session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3bb7e2-563c-4360-a26d-2208fed60d58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "INPUTMARKER_END = \"-- END --\"\n",
    "ANYSCALE_ENDPOINT_TOKEN = \"YOUR_ANYSCALE_ENDPOINT_TOKEN\"\n",
<<<<<<< HEAD:streaming_chatbot_langchain.ipynb
=======
<<<<<<< HEAD:streaming_chatbot_langchain.ipynb
    "ANYSCALE_ENDPOINT_TOKEN=\"secret_ramy6miafz45heuf3hff21babw\"\n",
=======
    "ANYSCALE_ENDPOINT_TOKEN = os.environ[\"ANYSCALE_API_KEY\"]\n",
>>>>>>> 0244ab3bcd4c8a3d2ddf49437428a76b23b058e9:Basic_streaming_chatbot_langchain.ipynb
>>>>>>> main:Basic_streaming_chatbot_langchain.ipynb
    "\n",
    "agent = LangchainChatAgent(\"meta-llama/Llama-2-70b-chat-hf\")\n",
    "sys.stdout.write(\"Let's have a chat. (Enter `quit` to exit)\\n\") \n",
    "while True:\n",
    "    sys.stdout.write('> ')\n",
    "    inp = input()\n",
    "    if inp == 'quit':\n",
    "        break\n",
    "    for word in agent.process_input(inp):\n",
    "        sys.stdout.write(word)\n",
    "        sys.stdout.flush()\n",
    "    sys.stdout.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
