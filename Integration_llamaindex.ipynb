{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ba0459f",
   "metadata": {},
   "source": [
    "# LlamaIndex Integration\n",
    "##### LlamaIndex is a data framework aimed at helping developers build LLM applications by providing essential tools that facilitate data ingestion, structuring, retrieval, and integration with various application frameworks. This example shows indexing and querying with LlamaIndex and requires the following packages: llama_index>=0.9.30, langchain>=0.0.257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c4df44-8e29-4326-a0f2-61232347de85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install -q llama_index langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0b24bd-665a-4b0f-ac46-7fe03528a4ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Load documents from 'data_folder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4fb005-eb38-4b9c-926e-075a90ff58ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader('data_folder').load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec66d8ad-fa96-49c3-a1f5-05bf14903f01",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Create a 'ServiceContext' using Anyscale support on LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3bb7e2-563c-4360-a26d-2208fed60d58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext, VectorStoreIndex\n",
    "from llama_index.llms import Anyscale\n",
    "from llama_index.embeddings import AnyscaleEmbedding\n",
    "\n",
    "ANYSCALE_ENDPOINT_TOKEN = \"YOUR_ANYSCALE_TOKEN\"\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=Anyscale(model = \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "                 api_key=ANYSCALE_ENDPOINT_TOKEN),\n",
    "    embed_model=AnyscaleEmbedding(model=\"thenlper/gte-large\",\n",
    "                                  api_key=ANYSCALE_ENDPOINT_TOKEN),\n",
    "    chunk_size=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dee5b44",
   "metadata": {},
   "source": [
    "### 3. Alternatively, you can build a similar LLM for the ServiceContext using 'ChatAnyscale' from LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f398d6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatAnyscale\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=ChatAnyscale(\n",
    "        anyscale_api_key=ANYSCALE_ENDPOINT_TOKEN,\n",
    "        model_name=\"meta-llama/Llama-2-70b-chat-hf\"),\n",
    "    embed_model=AnyscaleEmbedding(\n",
    "        model=\"thenlper/gte-large\",\n",
    "        api_key=ANYSCALE_ENDPOINT_TOKEN),\n",
    "    chunk_size=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01221ad",
   "metadata": {},
   "source": [
    "### 4. Create the index for documents with 'VectorStoreIndex' and query them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ff5024",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Sample Query Texts\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
