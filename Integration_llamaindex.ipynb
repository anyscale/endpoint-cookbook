{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ba0459f",
   "metadata": {},
   "source": [
    "# LlamaIndex Integration\n",
    "##### LlamaIndex is a data framework aimed at helping developers build LLM applications by providing essential tools that facilitate data ingestion, structuring, retrieval, and integration with various application frameworks. This example shows indexing and querying with LlamaIndex and requires the following packages: llama_index>=0.9.30, langchain>=0.0.257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c4df44-8e29-4326-a0f2-61232347de85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install -q llama_index langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0b24bd-665a-4b0f-ac46-7fe03528a4ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Load documents from 'data_folder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4fb005-eb38-4b9c-926e-075a90ff58ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader('data_folder').load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec66d8ad-fa96-49c3-a1f5-05bf14903f01",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Create a 'ServiceContext' using Anyscale support on LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3bb7e2-563c-4360-a26d-2208fed60d58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext, VectorStoreIndex\n",
    "from llama_index.llms import Anyscale\n",
    "from llama_index.embeddings import AnyscaleEmbedding\n",
    "\n",
    "# Set global tokenizer\n",
    "from llama_index import set_global_tokenizer\n",
    "from transformers import LlamaTokenizerFast\n",
    "set_global_tokenizer(\n",
    "    LlamaTokenizerFast.from_pretrained(\"hf-internal-testing/llama-tokenizer\").encode\n",
    ")\n",
    "\n",
    "ANYSCALE_ENDPOINT_TOKEN = \"YOUR_ANYSCALE_TOKEN\"\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=Anyscale(model = \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "                 api_key=ANYSCALE_ENDPOINT_TOKEN),\n",
    "    embed_model=AnyscaleEmbedding(model=\"thenlper/gte-large\",\n",
    "                                  api_key=ANYSCALE_ENDPOINT_TOKEN),\n",
    "    chunk_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dee5b44",
   "metadata": {},
   "source": [
    "### 3. Alternatively, you can build a similar LLM for the ServiceContext using 'ChatAnyscale' from LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f398d6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatAnyscale\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=ChatAnyscale(\n",
    "        anyscale_api_key=ANYSCALE_ENDPOINT_TOKEN,\n",
    "        model_name=\"meta-llama/Llama-2-70b-chat-hf\"),\n",
    "    embed_model=AnyscaleEmbedding(\n",
    "        model=\"thenlper/gte-large\",\n",
    "        api_key=ANYSCALE_ENDPOINT_TOKEN),\n",
    "    chunk_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01221ad",
   "metadata": {},
   "source": [
    "### 4. Create the index for documents with 'VectorStoreIndex' and query them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ff5024",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "query_engine = index.as_query_engine()\n",
    "que = \"Sample Query Texts\"\n",
    "response = query_engine.query(que)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932e4de2-75d7-4793-a84d-8ced1fb8ee57",
   "metadata": {},
   "source": [
    "### 5. Run relevance evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82a7571-34af-4dcd-a490-5b45e9f27cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run following code in a python file to avoid Jupyter runtime error\n",
    "from llama_index.evaluation import RelevancyEvaluator\n",
    "evaluator_gpt4 = RelevancyEvaluator(service_context=service_context)\n",
    "eval_result = evaluator_gpt4.evaluate_response( \n",
    "        query=que, response=response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
